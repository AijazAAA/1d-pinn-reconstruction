{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2030cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright Â© 2021 United States Government as represented \n",
    "by the Administrator of the National Aeronautics and Space Administration.\n",
    "No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras.backend as K\n",
    "#import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "def generate_spacetime_coloc_linear(space, time, n_coll, n_tcoll):\n",
    "    \"\"\"\n",
    "    Returns tensorflow tensor (ncoll*nt_coll, ndim+1) of spacetime\n",
    "    linearly distributed between [space0, space1] and [time0, time1]\n",
    "    \"\"\"\n",
    "    ndim = len(space)\n",
    "\n",
    "    space_coll = np.expand_dims(np.linspace(space[0][0],space[0][1],n_coll),1)\n",
    "\n",
    "    if(ndim > 1):\n",
    "        space_coll = np.column_stack((space_coll, np.linspace(space[1][0],space[1][1],n_coll)))\n",
    "\n",
    "\n",
    "    if(ndim > 2):\n",
    "        space_coll = np.column_stack((space_coll, np.linspace(space[2][0],space[2][1],n_coll)))\n",
    "\n",
    "\n",
    "    time_coll = np.linspace(time[0],time[1],n_tcoll)\n",
    "\n",
    "    spacetime_coll = np.tile(space_coll, reps=[n_tcoll,1])\n",
    "    spacetime_coll = np.column_stack((spacetime_coll, np.zeros((spacetime_coll.shape[0],1))))\n",
    "\n",
    "    for i in range(n_tcoll):\n",
    "        spacetime_coll[(i)*n_coll:(i+1)*n_coll,-1] = time_coll[i]\n",
    "\n",
    "    return tf.convert_to_tensor(spacetime_coll, dtype=K.floatx())\n",
    "\n",
    "def generate_spacetime_coloc_rand(space, time, n_coll_dim, tensor=False):\n",
    "    \"\"\"\n",
    "    Given N-dimensional array of space begin and end coordinates (e.g. [x0,x1])\n",
    "    and 1D array of time begin and end ([t0,t1])\n",
    "    return tensorflow tensor of n_coll_dim**2 randomly sampled points (x,t)\n",
    "    with shape (n_coll_dim**N, N+1) \n",
    "    \"\"\"\n",
    "    ndim = len(space)\n",
    "    coloc_pts = lhsamp(ndim+1,n_coll_dim**(ndim+1)) #ndim + 1 time dim\n",
    "\n",
    "    for i in range(ndim):\n",
    "        coloc_pts[:,i] = coloc_pts[:,i]*(space[i][1] - space[i][0]) + space[i][0]\n",
    "\n",
    "    coloc_pts[:,-1] = coloc_pts[:,-1]*(time[1]-time[0])+time[0]\n",
    "    if tensor:\n",
    "        coloc_pts = tf.convert_to_tensor(coloc_pts, dtype=K.floatx())\n",
    "\n",
    "    return coloc_pts\n",
    "\n",
    "#taken from pydoe2._lhsclassic\n",
    "# Original Code: Abraham Lee, Michael Baudin, Maria Christopoulou, Yann Collette\n",
    "# and Jean-Marc Martinez\n",
    "# https://github.com/clicumu/pyDOE2/blob/master/pyDOE2/doe_lhs.py\n",
    "# under BSD-3 license\n",
    "def lhsamp(n, samples, randomstate=np.random.RandomState()):\n",
    "    # Generate the intervals\n",
    "    cut = np.linspace(0, 1, samples + 1)\n",
    "\n",
    "    # Fill points uniformly in each interval\n",
    "    u = randomstate.rand(samples, n)\n",
    "    a = cut[:samples]\n",
    "    b = cut[1:samples + 1]\n",
    "    rdpoints = np.zeros_like(u)\n",
    "    for j in range(n):\n",
    "        rdpoints[:, j] = u[:, j]*(b-a) + a\n",
    "\n",
    "    # Make the random pairings\n",
    "    H = np.zeros_like(rdpoints)\n",
    "    for j in range(n):\n",
    "        order = randomstate.permutation(range(samples))\n",
    "        H[:, j] = rdpoints[order, j]\n",
    "\n",
    "    return H\n",
    "\n",
    "# trainable tanh activation function\n",
    "# uses node-individual beta\n",
    "def modtanh(x,b):\n",
    "    return tf.math.tanh(b*x)\n",
    "\n",
    "#uses individual betas, one for each node\n",
    "class ModTanh_all(Layer):\n",
    "    def __init__(self, output_dim, beta=1.,trainable=False, **kwargs):\n",
    "        super(ModTanh_all, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.beta = beta\n",
    "        self.trainable = trainable\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.beta_fac = self.add_weight(\"beta\", shape=[1, self.output_dim], trainable=self.trainable, dtype=K.floatx(), initializer=tf.constant_initializer(self.beta))\n",
    "        #if self.trainable:\n",
    "            #self._trainable_weights.append(self.beta_fac)\n",
    "\n",
    "        super(ModTanh_all, self).build(input_shape)\n",
    "        self.built=True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        return modtanh(inputs, self.beta_fac)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[-1], self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'beta': self.get_weights()[0] if self.trainable else self.beta,\n",
    "                  'trainable': self.trainable}\n",
    "        base_config = super(ModTanh_all, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class MHD_nd(Model):\n",
    "\n",
    "    def __init__(self, gamma, input_shape_dim=(2,), output_dim=8, nh = 48, nlayers=4, lambda_phys=1., path=None, model_name=None, visc=0., do_visc=False, loss_type='MSE', dx = 0.001, dt = 0.001):\n",
    "        \"\"\"\n",
    "        NOTE: This will save model information and checkpoints in './mhd_models/mhdnd/'+model_name_+'/' by default.\n",
    "        Default model name is 'MHD_recon_'+str(nlayers)+'x'+str(nh)'\n",
    "        NN architecture defined as in paper\n",
    "        \"Neural Network Reconstruction of Plasma Space-Time\" by C.Bard and J. Dorelli (DOI: 10.3389/fspas.2021.732275)\n",
    "        \n",
    "        gamma : ratio of specific heats\n",
    "        input_shape_dim : tuple denoting number of dimensions of input spacetime\n",
    "        output_dim : number of output variables in plasma state vector U\n",
    "        nh = number of nodes per layer\n",
    "        nlayers : number of layers\n",
    "        lambda_phys : regularization parameter between data loss and phys loss\n",
    "                      (applied to phys loss)\n",
    "        path : (optional) folder to save model/training information/output\n",
    "        model_name : (optional) name of model; saved in path\n",
    "        visc : setting for viscosity term nu\n",
    "        do_visc : True/False\n",
    "        loss_type : 'MSE' or 'logcosh'\n",
    "        dx : resolution in space\n",
    "        dt : resolution in time\n",
    "        \"\"\"\n",
    "        \n",
    "        super(MHD_nd, self).__init__()\n",
    "        self.min_loss = 10.\n",
    "        self.ckpt_min_loss = 10.\n",
    "        self.do_visc=do_visc\n",
    "        self.gamma = gamma\n",
    "        self.input_shape_dim = input_shape_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.tosave = True\n",
    "        self.visc = tf.constant(visc, dtype=K.floatx())\n",
    "        self.dx_arr = tf.constant((dx,0), dtype=K.floatx())\n",
    "        self.dt_arr = tf.constant((0,dt), dtype=K.floatx())\n",
    "        self.dx = tf.constant(dx, dtype=K.floatx())\n",
    "        self.dt = tf.constant(dt, dtype=K.floatx())\n",
    "        self.lambda_phys = tf.constant(lambda_phys,dtype=K.floatx())\n",
    "        self.optimizer = None\n",
    "\n",
    "        if loss_type.lower() == 'logcosh':\n",
    "            self.loss_type = self.loss_logcosh\n",
    "        else:\n",
    "            self.loss_type = self.loss_MSE\n",
    "\n",
    "        # interconnected network (Wang et al. 2020)\n",
    "        self.U = Dense(nh, input_shape=input_shape_dim, dtype=K.floatx())\n",
    "        self.U_act = ModTanh_all(nh, beta=1., trainable=True, dtype=K.floatx())\n",
    "\n",
    "        self.V = Dense(nh, input_shape=input_shape_dim, dtype=K.floatx())\n",
    "        self.V_act = ModTanh_all(nh, beta=1., trainable=True, dtype=K.floatx())\n",
    "\n",
    "        self.base_layers = []\n",
    "        self.act_layers = []\n",
    "        self.base_layers.append(Dense(nh, input_shape=input_shape_dim, dtype=K.floatx()))\n",
    "        self.act_layers.append(ModTanh_all(nh, beta=1., trainable=True, dtype=K.floatx()))\n",
    "\n",
    "        for i in range(nlayers-1):\n",
    "            self.base_layers.append(Dense(nh, input_shape=(nh,), dtype=K.floatx()))\n",
    "            self.act_layers.append(ModTanh_all(nh,beta=1., trainable=True, dtype=K.floatx()))\n",
    "\n",
    "        self.nlayers =nlayers\n",
    "\n",
    "        #output layer\n",
    "        self.output_layer = Dense(output_dim, input_shape=(nh,), activation = 'linear', dtype=K.floatx())\n",
    "\n",
    "        # metrics\n",
    "        if model_name is None:\n",
    "                model_name_ = 'MHD_recon_'+str(nlayers)+'x'+str(nh)\n",
    "            else:\n",
    "                model_name_ = model_name\n",
    "        if path is None:\n",
    "            self.path = './mhd_models/mhdnd/'+model_name_+'/'\n",
    "\n",
    "        else:\n",
    "            self.path = path+'/'+model_name+'/'\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        Ux = self.U(x)\n",
    "        Ux = self.U_act(Ux)\n",
    "        Vx = self.V(x)\n",
    "        Vx = self.V_act(Vx)\n",
    "        x = self.base_layers[0](x)\n",
    "        x = self.act_layers[0](x)\n",
    "        for base_layer,act_layer in zip(self.base_layers[1:], self.act_layers[1:]):\n",
    "            x = base_layer(x)\n",
    "            x = act_layer(x)\n",
    "            x = (1.-x)*Ux + x*Vx\n",
    "\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "    def loss(self, vd, vp, sc):\n",
    "        return self.loss_deriv(vd,vp,sc)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_MSE(residual):\n",
    "        return tf.reduce_mean(tf.square(residual))\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_logcosh(residual):\n",
    "        return tf.reduce_mean(tf.math.log(tf.math.cosh(residual)))\n",
    "\n",
    "    @tf.function\n",
    "    def loss_deriv(self, vec_data, vec_pred, space_coll):\n",
    "        #U_coll = []\n",
    "        dspace = []\n",
    "\n",
    "        if self.do_visc:\n",
    "            rho,vx,vy,vz,p,bx,by,bz,drho,dvx,dvy,dvz,dp,dbx,dby,dbz,d2vx = self.derivs_w_visc(space_coll)\n",
    "        else:\n",
    "            rho,vx,vy,vz,p,bx,by,bz,drho,dvx,dvy,dvz,dp,dbx,dby,dbz = self.derivs(space_coll)\n",
    "\n",
    "        # data/boundary MSE\n",
    "        # avg it by number of points, not samples\n",
    "        # i.e. sum(rho diff**2)/np + sum(vx diff**2)/np etc.\n",
    "        # and not sum(diff**2)/(np*8), which is what loss_MSE does\n",
    "        l0 = self.loss_type(vec_data-vec_pred)*self.output_dim\n",
    "\n",
    "        # d/dx: idx 0, d/dt: idx 1\n",
    "        #dU[:,idx]\n",
    "\n",
    "        #continuity eq: drho/dt + vx*drho/dx + rho*dvx/dx = 0\n",
    "        # and we want d/dt + d/dx = 0\n",
    "        space_deriv = drho[:,0]*vx + dvx[:,0]*rho\n",
    "        time_deriv = drho[:,1]\n",
    "\n",
    "        l1 = self.loss_type(time_deriv+space_deriv)*self.lambda_phys\n",
    "\n",
    "        # vx eq:\n",
    "        #issues with rho = 0, so multiply by rho to get\n",
    "        # rho*dvx/dt + rho*vx*dvx/dx + dp/dx + By*dBy/dx+ Bz*dBz/dx= 0\n",
    "        space_deriv = dvx[:,0]*vx*rho + dp[:,0] + by*dby[:,0] + bz*dbz[:,0]\n",
    "        time_deriv = rho*dvx[:,1]\n",
    "\n",
    "        # -nu*d2u/dx2 term\n",
    "        if self.do_visc:\n",
    "            space_deriv = space_deriv - self.visc*rho*d2vx[:]\n",
    "\n",
    "        l2 = self.loss_type(time_deriv+space_deriv)*self.lambda_phys\n",
    "\n",
    "        # vy equation:\n",
    "        # rho*dvy/dt + rho*vx*dvy/dx - Bx*dBy/dx = 0\n",
    "        space_deriv = rho*vx*dvy[:,0] - bx*dby[:,0]\n",
    "        time_deriv = rho*dvy[:,1]\n",
    "\n",
    "        l3 = self.loss_type(time_deriv+space_deriv)*self.lambda_phys\n",
    "\n",
    "        # vz equation:\n",
    "        # rho*dvz/dt + rho*vx*dvz/dx - Bx*dBz/dx = 0\n",
    "        space_deriv = rho*vx*dvz[:,0] - bx*dbz[:,0]\n",
    "        time_deriv = rho*dvz[:,1]\n",
    "\n",
    "        l4 = self.loss_type(time_deriv+space_deriv)*self.lambda_phys\n",
    "\n",
    "        # pressure eq: dp/dt + gamma*p*dvx/dx + vx*dp/dx\n",
    "        space_deriv = tf.scalar_mul(self.gamma,dvx[:,0]*p) + dp[:,0]*vx\n",
    "        time_deriv = dp[:,1]\n",
    "\n",
    "        l5 = self.loss_type(time_deriv+space_deriv)*self.lambda_phys\n",
    "\n",
    "        #bx eq: dBx/dt = d/dy(...) + d/dz(...) = 0 by 1D formulation\n",
    "        # also, divergence requirements -> d/dx(Bx) =0\n",
    "        l6a = self.loss_type(dbx[:,0])*self.lambda_phys\n",
    "        l6b = self.loss_type(dbx[:,1])*self.lambda_phys\n",
    "\n",
    "        # by eq\n",
    "        # dBy/dt + vx*dBy/dx + By*dvx/dx - Bx*dvy/dx (-vy*dBx/dx) = 0\n",
    "        space_deriv = vx*dby[:,0] + by*dvx[:,0] - bx*dvy[:,0]\n",
    "        time_deriv = dby[:,1]\n",
    "\n",
    "        l7 = self.loss_type(time_deriv+space_deriv)*self.lambda_phys\n",
    "        \n",
    "        #bz eq\n",
    "        # dBz/dt + vx*dBz/dx + Bz*dvx/dx - Bx*dvz/dx (-vz*dBx/dx) = 0\n",
    "        space_deriv = vx*dbz[:,0] + bz*dvx[:,0] - bx*dvz[:,0]\n",
    "        time_deriv = dbz[:,1]\n",
    "\n",
    "        l8 = self.loss_type(time_deriv+space_deriv)*self.lambda_phys\n",
    "        \n",
    "        #tot = l0+l1+l2+l3+l4+l5+l6a+l6b+l7+l8\n",
    "        # note that tape.gradient(losses) will sum over\n",
    "        #  all dloss/dparam for loss in losses\n",
    "        return [l0,l1,l2,l3,l4,l5,l6a,l6b,l7,l8]\n",
    "\n",
    "    # without viscosity\n",
    "    def derivs(self, space_coll):\n",
    "        dx = self.dx\n",
    "        dt = self.dt\n",
    "\n",
    "        U_coll = self(space_coll, training=True)\n",
    "        U_coll_px = self(space_coll+self.dx_arr, training=True)\n",
    "        U_coll_mx = self(space_coll-self.dx_arr, training=True)\n",
    "        U_coll_pt = self(space_coll+self.dt_arr, training=True)\n",
    "        U_coll_mt = self(space_coll-self.dt_arr, training=True)\n",
    "\n",
    "        rho = U_coll[:,0]\n",
    "        vx = U_coll[:,1]\n",
    "        vy = U_coll[:,2]\n",
    "        vz = U_coll[:,3]\n",
    "        p = U_coll[:,4]\n",
    "        bx = U_coll[:,5]\n",
    "        by = U_coll[:,6]\n",
    "        bz = U_coll[:,7]\n",
    "\n",
    "        drho = tf.stack([(U_coll_px[:,0] - U_coll_mx[:,0])/(2*dx), (U_coll_pt[:,0] - U_coll_mt[:,0])/(2*dt)],axis=1)\n",
    "        dvx = tf.stack([(U_coll_px[:,1] - U_coll_mx[:,1])/(2*dx), (U_coll_pt[:,1] - U_coll_mt[:,1])/(2*dt)],axis=1)\n",
    "        dvy = tf.stack([(U_coll_px[:,2] - U_coll_mx[:,2])/(2*dx), (U_coll_pt[:,2] - U_coll_mt[:,2])/(2*dt)],axis=1)\n",
    "        dvz = tf.stack([(U_coll_px[:,3] - U_coll_mx[:,3])/(2*dx), (U_coll_pt[:,3] - U_coll_mt[:,3])/(2*dt)],axis=1)\n",
    "        dp = tf.stack([(U_coll_px[:,4] - U_coll_mx[:,4])/(2*dx), (U_coll_pt[:,4] - U_coll_mt[:,4])/(2*dt)],axis=1)\n",
    "        dbx = tf.stack([(U_coll_px[:,5] - U_coll_mx[:,5])/(2*dx), (U_coll_pt[:,5] - U_coll_mt[:,5])/(2*dt)],axis=1)\n",
    "        dby = tf.stack([(U_coll_px[:,6] - U_coll_mx[:,6])/(2*dx), (U_coll_pt[:,6] - U_coll_mt[:,6])/(2*dt)],axis=1)\n",
    "        dbz = tf.stack([(U_coll_px[:,7] - U_coll_mx[:,7])/(2*dx), (U_coll_pt[:,7 ] - U_coll_mt[:,7])/(2*dt)],axis=1)\n",
    "\n",
    "        return rho,vx,vy,vz,p,bx,by,bz,drho,dvx,dvy,dvz,dp,dbx,dby,dbz\n",
    "\n",
    "    # with viscosity for velocity\n",
    "    def derivs_w_visc(self,space_coll):\n",
    "        dx = self.dx\n",
    "        dt = self.dt\n",
    "\n",
    "        U_coll = self(space_coll, training=True)\n",
    "        U_coll_px = self(space_coll+self.dx_arr, training=True)\n",
    "        U_coll_mx = self(space_coll-self.dx_arr, training=True)\n",
    "        U_coll_pt = self(space_coll+self.dt_arr, training=True)\n",
    "        U_coll_mt = self(space_coll-self.dt_arr, training=True)\n",
    "\n",
    "        rho = U_coll[:,0]\n",
    "        vx = U_coll[:,1]\n",
    "        vy = U_coll[:,2]\n",
    "        vz = U_coll[:,3]\n",
    "        p = U_coll[:,4]\n",
    "        bx = U_coll[:,5]\n",
    "        by = U_coll[:,6]\n",
    "        bz = U_coll[:,7]\n",
    "\n",
    "        drho = tf.stack([(U_coll_px[:,0] - U_coll_mx[:,0])/(2*dx), (U_coll_pt[:,0] - U_coll_mt[:,0])/(2*dt)],axis=1)\n",
    "        dvx = tf.stack([(U_coll_px[:,1] - U_coll_mx[:,1])/(2*dx), (U_coll_pt[:,1] - U_coll_mt[:,1])/(2*dt)],axis=1)\n",
    "        dvy = tf.stack([(U_coll_px[:,2] - U_coll_mx[:,2])/(2*dx), (U_coll_pt[:,2] - U_coll_mt[:,2])/(2*dt)],axis=1)\n",
    "        dvz = tf.stack([(U_coll_px[:,3] - U_coll_mx[:,3])/(2*dx), (U_coll_pt[:,3] - U_coll_mt[:,3])/(2*dt)],axis=1)\n",
    "        dp = tf.stack([(U_coll_px[:,4] - U_coll_mx[:,4])/(2*dx), (U_coll_pt[:,4] - U_coll_mt[:,4])/(2*dt)],axis=1)\n",
    "        dbx = tf.stack([(U_coll_px[:,5] - U_coll_mx[:,5])/(2*dx), (U_coll_pt[:,5] - U_coll_mt[:,5])/(2*dt)],axis=1)\n",
    "        dby = tf.stack([(U_coll_px[:,6] - U_coll_mx[:,6])/(2*dx), (U_coll_pt[:,6] - U_coll_mt[:,6])/(2*dt)],axis=1)\n",
    "        dbz = tf.stack([(U_coll_px[:,7] - U_coll_mx[:,7])/(2*dx), (U_coll_pt[:,7 ] - U_coll_mt[:,7])/(2*dt)],axis=1)\n",
    "\n",
    "        # only x derivative is used\n",
    "        d2vx = (U_coll_px[:,1] - 2*vx + U_coll_mx[:,1])/(dx*dx)\n",
    "\n",
    "        return rho,vx,vy,vz,p,bx,by,bz,drho,dvx,dvy,dvz,dp,dbx,dby,dbz,d2vx\n",
    "\n",
    "    def train(self, space_vec, data_vec, n_coll_dim, space_range, time_range, epochs=80000, randomize_ep=1, lr_decay=0.75, warmup_eps=0, nc_prog=[30,40,50,60,70], anneal_eps = 8500):\n",
    "        \"\"\"\n",
    "        space_vec: input location tensor; shape (n_pts, 2); for bounding positions in spacetime\n",
    "        data_vec : input data tensor; shape (n_pts, n_output_dim); for bounding plasma data\n",
    "        n_coll_dim : input N; used to generate N^2 random collocation points\n",
    "        space_range : list of lists; [[x_left, x_right]]; space extent of reconstruction domain\n",
    "        time_range : list; [t_0, t_1]; time extent of reconstruction domain\n",
    "        epochs : maximum number of training steps\n",
    "        randomize_ep : how often to randomize collocation point distribution\n",
    "        lr_decay : factor to reduce learning rate after max collocation points reached\n",
    "        warmup_eps : number of training steps to \"warmup\" network; \n",
    "                     training schedule starts after this period\n",
    "        nc_prog : list; denotes increments of n_coll_dim of training schedule\n",
    "        anneal_eps : duration of each training period with each n_coll_dim in nc_prog\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.log_path = self.path+'/log_'+current_time\n",
    "        os.makedirs(self.log_path)\n",
    "        self.summary_writer = tf.summary.create_file_writer(self.log_path)\n",
    "\n",
    "        progress = []\n",
    "\n",
    "        spacetime_coloc = generate_spacetime_coloc_rand(space_range, time_range, n_coll_dim, tensor=True)\n",
    "        spacetime_coloc = tf.concat((spacetime_coloc, space_vec),axis=0)\n",
    "\n",
    "        template = 'Epoch {}, Data Loss: {}, Physics Loss: {}'\n",
    "\n",
    "        if self.optimizer is None:\n",
    "            self.optimizer = tf.keras.optimizers.Adam(lr=1e-3, epsilon=1e-4)\n",
    "\n",
    "        lr_holder = self.optimizer.learning_rate.numpy()\n",
    "        lr_holder_orig = lr_holder\n",
    "        ckpt_no = 0\n",
    "        itw = 0\n",
    "        anneal = True\n",
    "\n",
    "        ftname = self.log_path+'/timing_notes.txt'\n",
    "        with open(ftname, 'w') as ftiming:\n",
    "            ftiming.write(\"Timings, start ncoll: {}\\n\".format(n_coll_dim))\n",
    "\n",
    "        start = timer()\n",
    "        for epoch in range(1,epochs):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predict_vec = self(space_vec, training = True)\n",
    "                losses = self.loss(data_vec, predict_vec, spacetime_coloc)\n",
    "\n",
    "            grad_loss = tape.gradient(losses, self.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grad_loss, self.trainable_variables))\n",
    "\n",
    "            #metrics\n",
    "            with self.summary_writer.as_default():\n",
    "                tf.summary.scalar('data loss', losses[0], step=epoch)\n",
    "                tf.summary.scalar('phys loss', sum(losses[1:]), step=epoch)\n",
    "                tf.summary.scalar('cont loss', losses[1], step=epoch)\n",
    "                tf.summary.scalar('vx loss', losses[2], step=epoch)\n",
    "                tf.summary.scalar('vy loss', losses[3], step=epoch)\n",
    "                tf.summary.scalar('vz loss', losses[4], step=epoch)\n",
    "                tf.summary.scalar('press eq loss', losses[5], step=epoch)\n",
    "                tf.summary.scalar('bxtime loss', losses[6], step=epoch)\n",
    "                tf.summary.scalar('bxspace loss', losses[7], step=epoch)\n",
    "                tf.summary.scalar('by loss', losses[8], step=epoch)\n",
    "                tf.summary.scalar('bz loss', losses[9], step=epoch)\n",
    "\n",
    "            if(epoch < 10):\n",
    "                print(template.format(epoch, losses[0].numpy(), tf.math.reduce_sum(losses[1:]).numpy()))\n",
    "            if(epoch%1000 == 0):\n",
    "                print(template.format(epoch, losses[0].numpy(), tf.math.reduce_sum(losses[1:]).numpy())+\" nc:{}\".format(n_coll_dim))\n",
    "\n",
    "            if(epoch%randomize_ep == 0):\n",
    "                # reset coloc points\n",
    "                spacetime_coloc = generate_spacetime_coloc_rand(space_range, time_range, n_coll_dim, tensor=True)\n",
    "                spacetime_coloc = tf.concat((spacetime_coloc, space_vec),axis=0)\n",
    "\n",
    "            if(epoch%5000==0):\n",
    "                self.save_self(self.log_path+'/ckpt{}_weights/'.format(ckpt_no))\n",
    "                ckpt_no += 1\n",
    "                print('checkpoint saved')\n",
    "                end = timer()\n",
    "                delta_t = timedelta(seconds=end-start)\n",
    "                with open(ftname, 'a') as ftiming:\n",
    "                    ftiming.write(\"epoch: {}, elapsed time: {}\\n\".format(epoch, delta_t))\n",
    "                start = timer()\n",
    "\n",
    "            if sum(losses).numpy() < self.min_loss:\n",
    "                self.min_loss = sum(losses).numpy()\n",
    "                self.save_self(self.log_path+'/best_weights/')\n",
    "\n",
    "            if(epoch > warmup_eps):\n",
    "                #start annealing\n",
    "                if(itw == 0):\n",
    "                    n_coll_dim = nc_prog[itw]\n",
    "                    itw += 1\n",
    "                    with open(ftname, 'a') as ftiming:\n",
    "                        ftiming.write(\"n coll dim now: {}\\n\".format(n_coll_dim))\n",
    "\n",
    "                if((epoch-warmup_eps)%1000 == 0):\n",
    "                    #check for stagnation\n",
    "                    print(\"loss check\", self.min_loss, self.ckpt_min_loss, (self.min_loss-self.ckpt_min_loss)/self.ckpt_min_loss)\n",
    "                    stag_check = (self.ckpt_min_loss-self.min_loss)/self.ckpt_min_loss\n",
    "                    self.ckpt_min_loss = self.min_loss\n",
    "\n",
    "                # Annealing: increase density of colocation points according to\n",
    "                # predefined annealing schedule\n",
    "                if(anneal and ((epoch-warmup_eps)+1)%anneal_eps == 0):\n",
    "                    self.load_weights(self.log_path+'/best_weights/weights')\n",
    "                    if itw < len(nc_prog):\n",
    "                        # go to next ncoll in progression\n",
    "                        n_coll_dim = nc_prog[itw]\n",
    "                        itw += 1\n",
    "                        with open(ftname, 'a') as ftiming:\n",
    "                            ftiming.write(\"n coll dim now: {}\\n\".format(n_coll_dim))\n",
    "                        print(\"n coll dim now\", n_coll_dim)\n",
    "\n",
    "                    else:\n",
    "                        #end of progression; decrease lr\n",
    "                        lr_holder = lr_holder * lr_decay\n",
    "                        self.optimizer.learning_rate = lr_holder\n",
    "                        with open(ftname, 'a') as ftiming:\n",
    "                            ftiming.write(\"learning rate now {}\\n\".format(lr_holder))\n",
    "                        print(\"learning rate now {}\".format(lr_holder))\n",
    "\n",
    "                    if lr_holder / lr_holder_orig < 0.2:\n",
    "                        print(\"learning rate minimum hit; ending training\")\n",
    "                        #anneal = False\n",
    "                        break\n",
    "\n",
    "        end = timer()\n",
    "        delta_t = timedelta(seconds=end-start)\n",
    "        with open(ftname, 'a') as ftiming:\n",
    "            ftiming.write(\"epoch: {}, elapsed time: {}\\n\".format(epoch, delta_t))\n",
    "        return 0\n",
    "\n",
    "    def save_self(self, path):\n",
    "        self.save_weights(path+'weights')\n",
    "\n",
    "    # warning: does not automatically load optimizer state\n",
    "    # what we need to do is to run new model (with same architecture)\n",
    "    # for some steps (like 1-2)\n",
    "    # and then we can call this to load the full state (including optimizer?)\n",
    "    def load_self(self, path):\n",
    "        self.load_weights(path+'weights')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
